{"kernelspec":{"display_name":"Anaconda (Python 3)","language":"python","name":"anaconda3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}}
{"cell_type":"markdown","metadata":{},"source":"<img style=\"float: left; width: 200px;\" src=\"../images/title1.png\">\n***\n# <font color=\"grey\">    Lecture 1 - What is optimization? </font>\n***"}
{"cell_type":"markdown","metadata":{},"source":">\"[N]othing at all takes place in the universe in which some rule of maximum or minimum does not appear.\" \n>\n>-- Leonhard Euler"}
{"cell_type":"markdown","metadata":{},"source":"**Mathematical optimization**, traditionally also known as mathematical programming, is the theory of optimal decision making. Optimization problems arise in a large variety of contexts, including **scheduling and logistics**, **finance**, **optimal control**, **signal processing**, and **machine learning**. The underlying mathematical problem always amounts to finding parameters that **minimize** (cost) or **maximize** (utility) an objective function in the presence or absence of a set of constraints.\nAn important special case is the class of **convex optimization** problems. Such problems will be the main focus of this course.\n$\\newcommand{\\vct}[1]{\\mathbf{#1}}$\n$\\newcommand{\\mtx}[1]{\\mathbf{#1}}$\n$\\newcommand{\\e}{\\varepsilon}$\n$\\newcommand{\\norm}[1]{\\|#1\\|}$\n$\\newcommand{\\minimize}{\\text{minimize}\\quad}$\n$\\newcommand{\\maximize}{\\text{maximize}\\quad}$\n$\\newcommand{\\subjto}{\\quad\\text{subject to}\\quad}$\n$\\newcommand{\\R}{\\mathbb{R}}$\n$\\newcommand{\\trans}{T}$\n$\\newcommand{\\ip}[2]{\\langle {#1}, {#2} \\rangle}$"}
{"cell_type":"markdown","metadata":{},"source":"---\n## <font color=\"grey\">What is an optimization problem?</font>\n---"}
{"cell_type":"markdown","metadata":{},"source":"A general mathematical optimization problem is a problem of the form\n\n\\begin{align*}\n\\begin{split}\n \\minimize & f(\\vct{x})\\\\\n \\subjto & \\vct{x}\\in \\Omega\n \\end{split}\n\\end{align*}\n\nwhere $f\\colon \\R^n\\to \\R$ is a real functions and $\\Omega\\subseteq \\R^n$ some set. Among all vectors $\\vct{x}\\in \\Omega$, we seek one with smallest $f$-value. The function $f(\\vct{x})$ is called the **objective function**, while $\\Omega$ are the **constraints**. A vector $\\vct{x}^*$ satisfying the constraints is called an **optimum**, a **solution**, or a **minimizer** of the problem, if $f(\\vct{x}^*)\\leq f(\\vct{x})$ for all other $\\vct{x}\\in \\Omega$. Note that replacing $f$ by $-f$, we could equivalently state the problem as a maximization problem. In this course we are mostly concerned with functions and constraint sets of a particular form.\n\n* A set $C\\subseteq \\R^n$ is **convex**, if for all $\\vct{x},\\vct{y}\\in C$ and $\\lambda\\in [0,1]$, $\\lambda \\vct{x}+(1-\\lambda)\\vct{y}\\in C$. That is, if for any two points in $C$, the line segment connecting them is also in $C$. \n* A function $f\\colon C\\to \\R$ is convex, if $C$ is convex and for all $\\vct{x}\\in C$ and $\\lambda\\in [0,1]$, $f(\\lambda \\vct{x}+(1-\\lambda)\\vct{y})\\leq \\lambda f(\\vct{x})+(1-\\lambda)f(\\vct{y})$. \n\n![Convex set](../images/convset.png)\n\nA **convex optimization** problem is one where the set of constraints $\\Omega$ and the function $f$ are convex."}
{"cell_type":"markdown","metadata":{},"source":"---\n## <font color=\"grey\">Examples of convex optimization problems</font>\n---\nCountless problems from science and engineering can be cast as convex optimization problems. We present a few first examples, many more will follow in the course of this lecture. The examples below come with associated Python code. At this moment it is not expected that you understand them in detail, they are intended to illustrate some of the problems that convex optimization deals with."}
{"cell_type":"markdown","metadata":{},"source":"### <font color=\"grey\">A first example: linear regression</font>\n---\n\nSuppose we want to understand the relationship of a quantity $Y$ (for example, sales data) to a series of *predictors* $X_1,\\dots,X_p$ (for example, advertising budget in different media). We can often assume the relationship to be *approximately linear*,\n\n[1]\\begin{equation*}\n Y = \\beta_0+\\beta_1 X_1 + \\cdots + \\beta_p X_p + \\varepsilon, \n\\end{equation*}\n\nwhere $\\varepsilon$ is some error or noise term. The goal is to determine the *model parameters* $\\beta_0,\\dots,\\beta_p$.\nTo determine these, we can collect $n\\geq p$ sample realizations (from observations or experiments),\n\n\\begin{equation*}\n Y=y_i, \\quad X_1=x_{i1},\\dots,X_p=x_{ip}, \\quad 1\\leq i\\leq n,\n\\end{equation*}\n\nand assume that the data is related according to [1], \n\n\\begin{equation*}\n y_i = \\beta_0+\\beta_1x_{i1}+\\cdots +\\beta_p x_{ip}+\\varepsilon_i, \\quad 1\\leq i\\leq n.\n\\end{equation*}\n\nCollecting the data in matrices and vectors,\n\n\\begin{equation*}\n \\vct{y} = \\begin{pmatrix}\n            y_1\\\\ \\vdots \\\\ y_n\n           \\end{pmatrix},\n\\quad \\mtx{X} = \\begin{pmatrix} \n           1 & x_{11} & \\cdots & x_{1p}\\\\\n           \\vdots & \\vdots & \\ddots & \\vdots \\\\\n           1 & x_{n1} & \\cdots & x_{np}\n          \\end{pmatrix},\n\\quad \\vct{\\beta} = \\begin{pmatrix}\n                     \\beta_0\\\\\n                     \\beta_1\\\\\n                     \\vdots\\\\\n                     \\beta_p\n                    \\end{pmatrix},\n\\quad \\vct{\\varepsilon} = \\begin{pmatrix}\n                  \\e_1\\\\\n                  \\vdots\\\\\n                  \\e_n\n                 \\end{pmatrix},\n\\end{equation*}\n\nwe can write the relationship concisely as \n\n\\begin{equation*}\n \\vct{y} = \\mtx{X}\\vct{\\beta}+\\vct{\\e}.\n\\end{equation*}\n\nWe would then like to find $\\vct{\\beta}$ in such a way that the difference $\\vct{\\e}=\\vct{y}-\\mtx{X}\\vct{\\beta}$ is as *small* as possible. One way of measuring the size of a vector $\\vct{x}\\in \\R^n$ is the square of its *2-norm*, or Euclidean norm, \n\n\\begin{equation*}\n \\norm{\\vct{x}}_2^2=\\vct{x}^{T}\\vct{x}=\\sum_{i=1}^nx_i^2.\n\\end{equation*}\n\nThe best $\\vct{\\beta}$ is then the vector that solves the unconstrained optimization problem\n\n\\begin{equation*}\n \\minimize \\norm{\\mtx{X}\\vct{\\beta}-\\vct{y}}_2^2.\n\\end{equation*}\n\nThis is an example of an optimization problem, with variables $\\vct{\\beta}$, no constraints (*all* $\\beta$ are valid candidates and the constraint set is $\\Omega=\\R^n$), and a *quadratic* objective function $f(\\vct{\\beta})=\\norm{\\mtx{X}\\vct{\\beta}-\\vct{y}}_2^2$. As we will see later, quadratic functions are convex, so this is a convex optimization problem.\nThis simple optimization problem has a *unique closed form solution*,\n\n\\begin{equation*}\n \\vct{\\beta}^* = (\\vct{X}^{\\trans}\\vct{X})^{-1}\\vct{X}^{\\trans}\\vct{y},\n\\end{equation*}\n\nwhere $\\vct{X}^{\\trans}$ is the matrix transpose. In practice one wouldn't compute $\\vct{\\beta}^*$ by evaluating [1], as there are more efficient methods available (see Lecture 2). "}
{"cell_type":"markdown","metadata":{},"source":"To illustrate the least squares setting using a concrete example, assume that we have data relating the basal metabolic rate (energy expenditure per time unit) in mammals to their mass (this example is from the episode ``Size Matters'' of the BBC series Wonders of Life.)\n\n![Brian Cox](../images/briancox-smaller.png)\n\nThe model we use is $Y=\\beta_0+\\beta_1X$, with $Y$ the basal metabolic rate and $X$ the mass. Using data for 573 mammals from the [PanTHERIA database](http://esapubs.org/archive/ecol/E090/184/\\#data), we can assemble the vector $\\vct{y}$ and the matrix $\\mtx{X}$ in order to compute the $\\vct{\\beta}=(\\beta_0,\\beta_1)^{\\trans}$.\n\nWe next illustrate how to solve this problem in Python. As usual, we first have to import some relevant libraries: **numpy** for numerical computation, **pandas** for loading and transforming datasets, **cvxpy** for convex optimization, and **matplotlib** for plotting."}
{"cell_type":"code","execution_count":1,"metadata":{"collapsed":true,"trusted":true},"outputs":[],"source":"# Import some important Python modules that we will be using\nimport numpy as np\nimport pandas as pd\nfrom cvxpy import *\nimport matplotlib.pyplot as plt"}
{"cell_type":"markdown","metadata":{},"source":"We next have to load the data. The data is saved in a table with 573 rows and 2 columns, where the first column list the mass and the second the basal metabolic rate."}
{"cell_type":"code","execution_count":2,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"(573, 2)"},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":"# Load data into numpy array\nbmr = pd.read_csv('../../data/bmr.csv',header=None).as_matrix()\n# We can find out the dimension of the data using the shape attribute\nbmr.shape"}
{"cell_type":"markdown","metadata":{},"source":"To see the first three and the last three rows of the dataset, we can use the \"print\" command."}
{"cell_type":"code","execution_count":3,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"[[ 13.108   10.604 ]\n [  9.3918   8.2158]\n [ 10.366    9.3285]]\n"}],"source":"print(bmr[0:3,:])"}
{"cell_type":"markdown","metadata":{},"source":"To visualise the whole dataset, we can make a scatterplot by interpreting each row as a coordinate on the plane, and marking it with a dot."}
{"cell_type":"code","execution_count":4,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"[<matplotlib.lines.Line2D at 0x7ff93f6d8198>]"},"execution_count":4,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"smc-blob::ed4f60be-b36e-4999-abba-3593850bce81","text/plain":"<matplotlib.figure.Figure at 0x7ff9439a1dd8>"},"metadata":{},"output_type":"display_data"}],"source":"# Display scatterplot of data (plot all the rows as points on the plane)\n% matplotlib inline\nplt.plot(bmr[:,0],bmr[:,1],'o')"}
{"cell_type":"markdown","metadata":{},"source":"The plot above suggests that the relation of the logarithm of the basal metabolic rate to the mass is linear, i.e., of the form\n\\begin{equation*}\n  Y = \\beta_0+\\beta_1 X,\n\\end{equation*}\nwhere X is the mass and Y the BMR. We can find $\\beta_0$ and $\\beta_1$ by solving an optimization problem as described above. We first have to assemble the matrix $\\mtx{X}$ and the vector $\\vct{y}$."}
{"cell_type":"code","execution_count":5,"metadata":{"collapsed":false,"trusted":true},"outputs":[],"source":"n = bmr.shape[0]\np = 1\nX = np.concatenate((np.ones((n,1)),bmr[:,0:p]),axis=1)\ny = bmr[:,-1]"}
{"cell_type":"code","execution_count":6,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"status:  optimal\noptimal value:  152.736200529558\noptimal variables:  1.3620698558275837 0.7016170245505547\n"}],"source":"# Method 1: using CVXPY\n\n# Create a (p+1) vector of variables\nBeta = Variable(p+1)\n\n# Create sum-of-squares objective function\nobjective = Minimize(sum_entries(square(X*Beta - y)))\n\n# Create problem and solve it\nprob = Problem(objective)\nprob.solve()\n\nprint(\"status: \", prob.status)\nprint(\"optimal value: \", prob.value)\nprint(\"optimal variables: \", Beta[0].value, Beta[1].value)"}
{"cell_type":"markdown","metadata":{},"source":"Now that we solved the problem and have the values $\\beta_0 = 1.362$ and $\\beta_1 = 0.702$, we can plot the line and see how it fits the data."}
{"cell_type":"code","execution_count":7,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"[<matplotlib.lines.Line2D at 0x7ff93f309908>]"},"execution_count":7,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"smc-blob::e9a43971-0178-4eb6-a996-2e4a4daae120","text/plain":"<matplotlib.figure.Figure at 0x7ff93f7a63c8>"},"metadata":{},"output_type":"display_data"}],"source":"plt.plot(bmr[:,0],bmr[:,1],'o')\n\nxx = np.linspace(0,14,100)\nplt.plot(xx, Beta[0].value+Beta[1].value*xx, color='black', linewidth=2)"}
{"cell_type":"markdown","metadata":{"collapsed":true},"source":"Even though for illustration purposes we used the CVXPY package, this particular problem can be solved directly using the least squares solver in numpy."}
{"cell_type":"code","execution_count":8,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"[ 1.36206997  0.70161692]\n"}],"source":"import numpy.linalg as la\nbeta = la.lstsq(X,y)\nprint(beta[0])"}
{"cell_type":"markdown","metadata":{},"source":"### <font color=\"grey\">Machine Learning</font>\n---\n\nThe above example is an example of a **machine learning** problem. In machine learning, one seeks to **learn** a function $F$ mapping some inputs $X$ to outputs $Y$, $Y=F(X)$. A few examples:\n* X: economic data, Y: value of a stock;\n* X: physiological data, Y: medical diagnosis;\n* X: email, Y: 1 if email is span, 0 otherwise;\n* X: scanned image, Y: a letter represented by that image.\n\nIn **supervised learning** we have a set of sample input pairs, $(y_i,x_i)$, $1\\leq i\\leq m$, and we typically try to find a function $F$ that minimizes the **least squared error**,\n\n\\begin{equation*}\n  \\minimize \\sum_{i=1}^m (\\vct{y}_i-F(\\vct{x}_i))^2,\n\\end{equation*}\n\nwhere one minimizes over all functions $F$ from some class. In the above example, we assumed our functions to be linear, in which case the can by parametrized by the coefficients $\\beta_0, \\dots,\\beta_p$. As the course progresses, we will see examples of more sophisticated machine learning problems, often with nonlinear objective function and other **loss functions** instead of the least square error. "}
{"cell_type":"markdown","metadata":{},"source":"### <font color=\"grey\">A second example: linear programming</font>\n---\nSuppose a plane has two cargo compartments with weight capacities $C_1=35$ and $C_2=40$ tonnes, and volumes (space capacities) $V_1=250$ and $V_2=400$ cubic metres. Assume we have three types of cargo to transport, specified as follows.\n \n |          | Volume (m$^3$ per tonne) | Weight (tonnes) | Profit (£ / tonne)|\n |---------:|--------------------------|-----------------|-------------------------| \n | Cargo 1  |   8                      |  25             | £ 300           |\n | Cargo 2  |  10                      |  32             | £ 350           |\n | Cargo 3  |   7                      |  28             | £ 270           |\n  \n The problem is now to decide how much of each cargo to take on board, and how to distribute it in an optimal way among the two compartments.\n 1. The **decision variables** $x_{ij}$ specify the amount, in tonnes, of cargo $i$ to go into compartment $j$. We collect them in a vector $\\vct{x}$.\n 2. The **objective function** is the total profit, \n \n  \\begin{equation*}\n   f(\\vct{x}) = 300\\cdot (x_{11}+x_{12})+ 350\\cdot (x_{21}+x_{22})+270\\cdot (x_{31}+x_{32}).\n  \\end{equation*}\n \n \n 3. The **constraints** are given by the space and weight limitations of the compartments, and the amount of cargo available.\n\n\\begin{align*}\n x_{11}+x_{12} & \\leq 25 \\quad \\text{ (total amount of cargo 1)}\\\\ \n x_{21}+x_{22} & \\leq 32 \\quad \\text{ (total amount of cargo 2)}\\\\ \n x_{31}+x_{32} & \\leq 28 \\quad \\text{ (total amount of cargo 3)}\\\\\n x_{11}+x_{21}+x_{31} & \\leq 35 \\quad \\text{ (weight constraint on compartment 1)}\\\\\n x_{12}+x_{22}+x_{32} & \\leq 40 \\quad \\text{ (weight constraint on compartment 2)}\\\\\n 8x_{11}+10x_{21}+7x_{31} & \\leq 250 \\quad \\text{ (volume constraint on compartment 1)}\\\\\n 8x_{12}+10x_{22}+7x_{32} & \\leq 400 \\quad \\text{ (volume constraint on compartment 2)}\\\\\n (x_{11}+x_{21}+x_{31})/35 - (x_{12}+x_{22}+x_{32})/40 &= 0 \\quad \\text{ (maintain balance of weight ratio)}\\\\\n x_{ij} &\\geq 0 \\quad \\text{ (cargo can't have negative weight)}\n\\end{align*}\n\n It is customary to write the objective function as a scalar product, $f(\\vct{x}) = \\ip{\\vct{c}}{\\vct{x}} := \\vct{c}^{\\trans}\\vct{x}$, and to express the constraints as systems of linear equations and inequalities using matrix-vector products,\n\n\\begin{align*}\n  \\maximize &\\ip{\\vct{c}}{\\vct{x}} \\\\\n  \\subjto &A\\vct{x}\\leq \\vct{b}\\\\\n  & B\\vct{x} = \\vct{d}\\\\\n          & \\vct{x}\\geq 0        \n \\end{align*}\n\nwhere the inequalities $\\geq$ and $\\leq$ are to be understood componentwise.\n This problem has a unique solution that can be found using CVXPY in Python,\n "}
{"cell_type":"code","execution_count":9,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Solution found: \n [[  6.75   7.71   0.    32.    28.     0.  ]]\n"}],"source":"# Define all the matrices and vectors involved\nc = np.array([300,300,350,350,270,270])\nA = np.array([[1, 1, 0, 0, 0, 0],\n              [0, 0, 1, 1, 0, 0],\n              [0, 0, 0, 0, 1, 1],\n              [1, 0, 1, 0, 1, 0],\n              [0, 1, 0, 1, 0, 1],\n              [8, 0, 10, 0, 7, 0],\n              [0, 8, 0, 10, 0, 7]])\nb = np.array([25,32,28,35,40,250,400]);\nB = np.array([1/35, -1/40, 1/35, -1/40, 1/35, -1/40]);\nd = np.zeros(1)\n\n# Create variables, objective and constraints\nx = Variable(6)\nconstraints = [A*x <= b, B*x == d, x >= 0]\nobjective = Maximize(c*x)\nprob = Problem(objective, constraints)\n\n# Solve the optimization problem and output the result\nprob.solve()\nprint(\"Solution found: \\n\", np.round(np.abs(x.value), decimals=2).transpose())"}
{"cell_type":"markdown","metadata":{},"source":"In summary, the solution found is\n \\begin{equation*}\n x_{11} = 6.75 , x_{12} =  7.71, x_{21} = 0, x_{22} = 32, x_{31} = 28, x_{32} = 0.\n \\end{equation*}\n\n We made some simplifying assumptions, for example that the cargo can be split up into arbitrary fractions. Additional work is required to resolve these issues.\n Problems of this kind are known as **linear programming**, because the objective function and the constraints are given by linear functions. Such problems can be solved efficiently using the simplex algorithm or interior point methods. The highly developed theory of linear programming acts as a template for more general convex optimization that is developed in this course."}
{"cell_type":"markdown","metadata":{},"source":"### <font color=\"grey\">A third example: image inpainting</font>\n---\nOptimization methods play an increasingly important role in image and signal processing. An image can be viewed as an $m\\times n$ matrix $\\mtx{U}$, with each entry corresponding to a light intensity (for greyscale images), or a colour vector, represented by a triple of red, green and blue intensities (usually with values between $0$ and $255$ each). For simplicity the following discussion assumes a greyscale image. For compututational pursposes, the matrix of an image is often viewed as an $mn$-dimensional vector $\\vct{u}$, with the columns of the matrix stacked on top of each other. \n\nIn the **image inpainting** problem, one aims to *guess* the *true* value of missing or corrupted entries of an image. There are different approaches to this problem. A conceptually simple approach is to replace the image with the *closest* image among a set of images satisfying typical properties. But what are typical properties of a typical image? Some properties that come to mind are:\n\n* Images tend to have large homogeneous areas in which the colour doesn't change much;\n* Images have approximately low rank, when interpreted as matrices.\n\nTotal variation image analysis takes advantage of the first property. The **total variation** or TV-norm is the sum of the norm of the horizontal and vertical differences,\n\n\\begin{equation*}\n  \\|\\vct{U}\\|_{\\mathrm{TV}} = \\sum_{i=1}^m \\sum_{j=1}^n \\sqrt{(u_{i+1,j}-u_{i,j})^2+(u_{i,j+1}-u_{i,j})^2},\n\\end{equation*}\n\nwhere we set entries with out-of-bounds indices to $0$. The TV-norm naturally increases with increased variation or sharp edges in an image. Consider for example the to matrices (imagine that they represent a $3\\times 3$ pixel block taken from an image).\n\n\\begin{equation*}\n\\mtx{U}_1 = \\begin{pmatrix}\n 0 & 17 & 3 \\\\\n 7 & 32 & 0 \\\\\n 2 & 9 & 27\n\\end{pmatrix}, \\quad\n\\mtx{U}_2\\begin{pmatrix}\n1 & 1 & 3\\\\\n1 & 0 & 0\\\\\n0 & 0 & 2\n\\end{pmatrix}\n\\end{equation*}\n\nThe left matrix has TV-norm $\\|\\mtx{U}_1\\|_{\\mathrm{TV}} = 200.637$, while the right one has TV-norm $\\|\\mtx{U}_2\\|_{\\mathrm{TV}} = 14.721$ (verify this!) Intuitively, we would expect a natural image with artifacts added to it to have a higher TV norm.\n\nNow let $\\mtx{U}$ be an image with entries $u_{ij}$, and let $\\Omega\\subset [m]\\times [n] = \\{(i,j) \\mid 1\\leq i\\leq m, 1\\leq j\\leq n\\}$ be the set of indices where the original image and the corrupted image coincide (all the other entries are missing). One could attempt to find the image with the *smallest* TV-norm that coincides with the knwon pixels $u_{ij}$ for $(i,j)\\in \\Omega$. This is an optimization problem of the form\n\n\\begin{equation*}\n  \\minimize \\|\\vct{X}\\|_{\\mathrm{TV}} \\subjto x_{ij} = u_{ij} \\text{ for } (i,j) \\in \\Omega.\n\\end{equation*}\n\nThe TV-norm is an example of a convex function and the constraints are linear conditions which define a convex set. This is again an example of a **convex optimization problem** and can be solved efficiently by a range of algorithms. For the time being we will not go into the algorithms but solve it using CVXPY. The example below is based on an example from the [CVXPY Tutorial](http://www.cvxpy.org/en/latest/tutorial/index.html), and it is recommended to look at that tutorial for more interesting examples!"}
{"cell_type":"markdown","metadata":{},"source":"In our first piece of code below, we load the image and a version of the image with text written on it, and display the images. The Python Image Library (PIL) is used for this purpose."}
{"cell_type":"code","execution_count":10,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"image/png":"smc-blob::957b6d2d-4264-406e-a466-dc24787344c7","text/plain":"<matplotlib.figure.Figure at 0x7ff93f7acd30>"},"metadata":{},"output_type":"display_data"}],"source":"from PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Load the images and convert to numpy arrays for processing.\nU = np.array(Image.open(\"../images/alanturing.png\"))\nUcorr = np.array(Image.open(\"../images/alanturing-corr.png\"))\n\n# Display the images\n%matplotlib inline\nfig, ax = plt.subplots(1, 2,figsize=(10, 5))\nax[0].imshow(U);\nax[0].set_title(\"Original Image\")\nax[0].axis('off')\nax[1].imshow(Ucorr);\nax[1].set_title(\"Corrupted Image\")\nax[1].axis('off');"}
{"cell_type":"markdown","metadata":{},"source":"After having the images at our disposal, we determine which entries of the corrupted image are known. We store these in a *mask* $M$, with entries $m_{ij}=1$ if the $(i,j)$-th pixel is known, and $0$ otherwise."}
{"cell_type":"code","execution_count":11,"metadata":{"collapsed":false,"trusted":true},"outputs":[],"source":"# Each image is now an m x n x 3 array, with each pixel represented\n# by three numbers between 0 and 255, corresponding to red, green and blue\nrows, cols, colours = U.shape\n\n# Create a mask: this is a matrix with a 1 if the corresponding \n# pixel is known, and zero else\nM = np.zeros((rows, cols, colours))\nfor i in range(rows):\n    for j in range(cols):\n        for k in range(colours):\n            if U[i, j, k] == Ucorr[i, j, k]:\n                M[i, j, k] = 1"}
{"cell_type":"markdown","metadata":{},"source":"We are now ready to solve the optimization problem using CVXPY. As the problem is rather big ($400\\times 600\\times 3 = 720000$ variables), it is important to choose a good solver that will solve the problem to sufficient accuracy in an acceptable amount of time. For the example at hand, we choose the SCS solver, which can be specified when calling the **solve** function."}
{"cell_type":"code","execution_count":12,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"----------------------------------------------------------------------------\n\tSCS v1.2.6 - Splitting Conic Solver\n\t(c) Brendan O'Donoghue, Stanford University, 2012-2016\n----------------------------------------------------------------------------\nLin-sys: sparse-indirect, nnz in A = 3774831, CG tol ~ 1/iter^(2.00)\neps = 1.00e-03, alpha = 1.50, max_iters = 2500, normalize = 1, scale = 1.00\nVariables n = 959001, constraints m = 2393007\nCones:\tprimal zero / dual free vars: 720000\n\tsoc vars: 1673007, soc blks: 239001\nSetup time: 2.90e-01s\n----------------------------------------------------------------------------\n Iter | pri res | dua res | rel gap | pri obj | dua obj | kap/tau | time (s)\n----------------------------------------------------------------------------\n     0| 6.10e+00  5.64e+00  1.00e+00 -2.62e+08  1.57e+07  4.12e-08  3.46e+00 \n   100| 2.55e-03  7.42e-04  1.12e-03  8.23e+06  8.25e+06  8.60e-09  5.17e+01 \n   200| 8.29e-04  1.91e-04  2.45e-04  8.26e+06  8.27e+06  8.74e-09  1.01e+02 \n----------------------------------------------------------------------------\nStatus: Solved\nTiming: Solve time: 1.01e+02s\n\tLin-sys: avg # CG iterations: 8.82, avg solve time: 4.25e-01s\n\tCones: avg projection time: 5.85e-03s\n----------------------------------------------------------------------------\nError metrics:\ndist(s, K) = 8.5265e-14, dist(y, K*) = 2.2204e-16, s'y/|s||y| = -5.5397e-18\n|Ax + s - b|_2 / (1 + |b|_2) = 8.2942e-04\n|A'y + c|_2 / (1 + |c|_2) = 1.9150e-04\n|c'x + b'y| / (1 + |c'x| + |b'y|) = 2.4531e-04\n----------------------------------------------------------------------------\nc'x = 8263910.8123, -b'y = 8267966.1911\n============================================================================\n"},{"data":{"text/plain":"8263910.812250629"},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":"# Recover the original image using total variation in-painting.\nimport cvxpy as cvx\n\n# Determine the variables and constraints\nvariables = []\nconstraints = []\nfor k in range(colours):\n    X = cvx.Variable(rows, cols)\n    # Add variables\n    variables.append(X)\n    # Add constraints by multiplying the relevant variable matrix elementwise with the mask\n    constraints.append(cvx.mul_elemwise(M[:, :, k], X) == cvx.mul_elemwise(M[:, :, k], Ucorr[:, :, k]))\n\n# Create a problem instance with\nobjective = cvx.Minimize(cvx.tv(variables[0],variables[1],variables[2]))\n\n# Create a problem instance and solve it using the SCS solver\nprob = cvx.Problem(objective, constraints)\nprob.solve(verbose=True, solver=cvx.SCS)"}
{"cell_type":"markdown","metadata":{},"source":"Now that we solved the optimization problem, we have a solution stored in 'variables'. We have to transform this back into an image and display the result."}
{"cell_type":"code","execution_count":13,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"image/png":"smc-blob::aacc205e-bb5b-4cf5-be87-1a705cf3cc3e","text/plain":"<matplotlib.figure.Figure at 0x7ff937b67a58>"},"metadata":{},"output_type":"display_data"}],"source":"%matplotlib inline\n\n# Load variable values into a single array.\nUrec = np.zeros((rows, cols, colours), dtype=np.uint8)\nfor i in range(colours):\n    Urec[:, :, i] = variables[i].value\n\nfig, ax = plt.subplots(1, 2,figsize=(10, 5))\n\n# Display the inpainted image.\nax[0].imshow(Urec);\nax[0].set_title(\"Inpainted Image\")\nax[0].axis('off')\n\nax[1].imshow(np.abs(Ucorr[:,:,0:3] - Urec));\nax[1].set_title(\"Difference Image\")\nax[1].axis('off');"}
{"cell_type":"markdown","metadata":{"collapsed":true},"source":"Another typical structure of images is that the **singular values** of the image, considered as matrix, decay quickly. The **singular value decomposition** (SVD) of a matrix $\\mtx{A}\\in \\R^{m\\times n}$ is the matrix product\n\n\\begin{equation*}\n  \\mtx{A} = \\mtx{U}\\mtx{\\Sigma}\\mtx{V}^{T},\n\\end{equation*}\n\nwhere $\\mtx{U}\\in \\R^{m\\times m}$ and $\\mtx{V}\\in \\R^{n\\times n}$ are orthogonal matrices, and $\\mtx{\\Sigma}\\in \\R^{m\\times n}$ is a diagonal matrix with entries $\\sigma_{1},\\dots,\\sigma_{\\mathrm{min}\\{m,n\\}}$ on the diagonal. Instead of minimizing the TV-norm of an image $\\mtx{X}$, one may instead try to minimize the **Schatten 1-norm**, defined as the sum of the singular values, $\\|\\vct{U}\\|_{S_1} = \\sigma_1+\\cdots+\\sigma_{\\mathrm{min}\\{m,n\\}}$. The problem is then\n\n\\begin{equation*}\n  \\minimize \\|\\vct{X}\\|_{S_1} \\subjto x_{ij} = u_{ij} \\text{ for } (i,j) \\in \\Omega.\n\\end{equation*}\n\nAs we will see towards the end of the course, this is an instance of a type of convex optimization problem known as **semidefinite programming**. Luckily, CVXPY includes the Schatten 1-norm (also known as nuclear norm) as valid objective function, so we don't have to deal with the details of this problem. As the problem is computationally intensive, we just reproduce the result.\n\n![Nuclear norm inpainting](../images/nucnorm-inpaint.png)\n\nThe result does not look as promising at first sight as using the TV norm."}
{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"trusted":true},"outputs":[],"source":""}