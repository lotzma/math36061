{"kernelspec":{"display_name":"Anaconda (Python 3)","language":"python","name":"anaconda3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}}
{"cell_type":"markdown","metadata":{},"source":"<img style=\"float: left; width: 200px;\" src=\"./images/title1.png\">\n***\n# <font color=\"grey\">    Lecture 1 - What is optimization? </font>\n***"}
{"cell_type":"markdown","metadata":{},"source":">\"[N]othing at all takes place in the universe in which some rule of maximum or minimum does not appear.\" \n>\n>-- Leonhard Euler"}
{"cell_type":"markdown","metadata":{},"source":"**Mathematical optimization**, traditionally also known as mathematical programming, is the theory of optimal decision making. Optimization problems arise in a large variety of contexts, including **scheduling and logistics**, **finance**, **optimal control**, **signal processing**, and **machine learning**. The underlying mathematical problem always amounts to finding parameters that **minimize** (cost) or **maximize** (utility) an **objective function** in the presence or absence of **constraints**. An important special case is the class of **convex optimization** problems, which will be the main focus of this course.\n$\\newcommand{\\vct}[1]{\\mathbf{#1}}$\n$\\newcommand{\\mtx}[1]{\\mathbf{#1}}$\n$\\newcommand{\\e}{\\varepsilon}$\n$\\newcommand{\\norm}[1]{\\|#1\\|}$\n$\\newcommand{\\minimize}{\\text{minimize}\\quad}$\n$\\newcommand{\\maximize}{\\text{maximize}\\quad}$\n$\\newcommand{\\subjto}{\\quad\\text{subject to}\\quad}$\n$\\newcommand{\\R}{\\mathbb{R}}$\n$\\newcommand{\\trans}{T}$\n$\\newcommand{\\ip}[2]{\\langle {#1}, {#2} \\rangle}$"}
{"cell_type":"markdown","metadata":{},"source":"### <font color=\"grey\">A first example: linear regression</font>\n---\n\nSuppose we want to understand the relationship of a quantity $Y$ (for example, sales data) to a series of *predictors* $X_1,\\dots,X_p$ (for example, advertising budget in different media). We can often assume the relationship to be *approximately linear*,\n\n[1]\\begin{equation*}\n Y = \\beta_0+\\beta_1 X_1 + \\cdots + \\beta_p X_p + \\varepsilon, \n\\end{equation*}\n\nwhere $\\varepsilon$ is some error or noise term. The goal is to determine the *model parameters* $\\beta_0,\\dots,\\beta_p$.\nTo determine these, we can collect $n\\geq p$ sample realizations (from observations or experiments),\n\\begin{equation*}\n Y=y_i, \\quad X_1=x_{i1},\\dots,X_p=x_{ip}, \\quad 1\\leq i\\leq n,\n\\end{equation*}\nand assume that the data is related according to [1], \n\\begin{equation*}\n y_i = \\beta_0+\\beta_1x_{i1}+\\cdots +\\beta_p x_{ip}+\\varepsilon_i, \\quad 1\\leq i\\leq n.\n\\end{equation*}\nCollecting the data in matrices and vectors,\n\\begin{equation*}\n \\vct{y} = \\begin{pmatrix}\n            y_1\\\\ \\vdots \\\\ y_n\n           \\end{pmatrix},\n\\quad \\mtx{X} = \\begin{pmatrix} \n           1 & x_{11} & \\cdots & x_{1p}\\\\\n           \\vdots & \\vdots & \\ddots & \\vdots \\\\\n           1 & x_{n1} & \\cdots & x_{np}\n          \\end{pmatrix},\n\\quad \\vct{\\beta} = \\begin{pmatrix}\n                     \\beta_0\\\\\n                     \\beta_1\\\\\n                     \\vdots\\\\\n                     \\beta_p\n                    \\end{pmatrix},\n\\quad \\vct{\\varepsilon} = \\begin{pmatrix}\n                  \\e_1\\\\\n                  \\vdots\\\\\n                  \\e_n\n                 \\end{pmatrix},\n\\end{equation*}\nwe can write the relationship concisely as \n\\begin{equation*}\n \\vct{y} = \\mtx{X}\\vct{\\beta}+\\vct{\\e}.\n\\end{equation*}\nWe would then like to find $\\vct{\\beta}$ in such a way that the difference $\\vct{\\e}=\\vct{y}-\\mtx{X}\\vct{\\beta}$ is as *small* as possible. One way of measuring the size of a vector $\\vct{x}\\in \\R^n$ is the square of its *2-norm*, or Euclidean norm, \n\\begin{equation*}\n \\norm{\\vct{x}}_2^2=\\vct{x}^{T}\\vct{x}=\\sum_{i=1}^nx_i^2.\n\\end{equation*}\nThe best $\\vct{\\beta}$ is then the vector that solves the unconstrained optimization problem\n\\begin{equation*}\n \\minimize \\norm{\\mtx{X}\\vct{\\beta}-\\vct{y}}_2^2.\n\\end{equation*}\nThis is an example of an optimization problem, with variables $\\vct{\\beta}$, no constraints (*all* $\\beta$ are valid candidates), and a *quadratic* objective function $f(\\vct{\\beta})=\\norm{\\mtx{X}\\vct{\\beta}-\\vct{y}}_2^2$.\nThis simple optimization problem has a *unique closed form solution*,\n\\begin{equation*}\n \\vct{\\beta}^* = (\\vct{X}^{\\trans}\\vct{X})^{-1}\\vct{X}^{\\trans}\\vct{y},\n\\end{equation*}\nwhere $\\vct{X}^{\\trans}$ is the matrix transpose. In practice one wouldn't compute $\\vct{\\beta}^*$ by evaluating [1], as there are more efficient methods available (see Lecture 2)."}
{"cell_type":"code","execution_count":50,"metadata":{"collapsed":true,"trusted":true},"outputs":[],"source":"# Import some important Python modules that we will be using\nimport numpy as np\nimport pandas as pd\nimport cvxpy as cvx\nimport matplotlib.pyplot as plt\n%matplotlib inline"}
{"cell_type":"code","execution_count":43,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"(573, 2)"},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":"# Load data into numpy array\nbmr = pd.read_csv('../data/bmr.csv',header=None).as_matrix()\n# Figure out dimensions of data\nbmr.shape"}
{"cell_type":"markdown","metadata":{},"source":"The variable *bmr* is a 573x2 matrix containing pairs of basal metabolic rate and mass for 573 different mammals."}
{"cell_type":"code","execution_count":45,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"array([[ 13.108 ,  10.604 ],\n       [  9.3918,   8.2158],\n       [ 10.366 ,   9.3285]])"},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":"# Display first 3 rows of bmr\nbmr[0:3,:]"}
{"cell_type":"code","execution_count":46,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"data":{"text/plain":"[<matplotlib.lines.Line2D at 0x7f246d71cc18>]"},"execution_count":46,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"smc-blob::6592ccbb-13dd-4e9b-9538-85f89f4694a3","text/plain":"<matplotlib.figure.Figure at 0x7f2473025ba8>"},"metadata":{},"output_type":"display_data"}],"source":"# Display scatterplot of data (plot all the rows as points on the plane)\nplt.plot(bmr[:,0],bmr[:,1],'o')"}
{"cell_type":"markdown","metadata":{},"source":"The plot above suggests that the relation of the logarithm of the basal metabolic rate to the mass is linear, i.e., of the form\n\\begin{equation*}\n  Y = \\beta_0+\\beta_1 X,\n\\end{equation*}\nwhere X is the mass and Y the BMR. We can find $\\beta_0$ and $\\beta_1$ by solving an optimization problem as described above."}
{"cell_type":"code","execution_count":47,"metadata":{"collapsed":true,"trusted":true},"outputs":[],"source":"# Method 1: use standard optimization software"}
{"cell_type":"code","execution_count":48,"metadata":{"collapsed":true,"trusted":true},"outputs":[],"source":"# Method 2: solve normal equations"}
{"cell_type":"code","execution_count":49,"metadata":{"collapsed":true,"trusted":true},"outputs":[],"source":"# Method 3: use built-in linear algebra methods for solving a least-squares problem"}
{"cell_type":"markdown","metadata":{},"source":"### <font color=\"grey\">Machine Learning</font>\n---\n\nThe above example is an example of a **machine learning** problem. In machine learning, one seeks to **learn** a function $F$ mapping some inputs $X$ to outputs $Y$, $Y=F(X)$. A few examples:\n* X: something, Y: something else\n* X: grid of pixels, Y: a letter represented by those pixels;\nIn **supervised learning** we have a set of sample input pairs, $(y_i,x_i)$, $1\\leq i\\leq m$, and we typically try to find a function $F$ that minimizes the **least squared error**,\n\n\\begin{equation*}\n  \\minimize \\sum_{i=1}^m (\\vct{y}_i-F(\\vct{x}_i))^2,\n\\end{equation*}\n\nwhere one minimizes over all functions $F$ from some class. In the above example, we assumed our functions to be linear, in which case the can by parametrized by the coefficients $\\beta_0, \\dots,\\beta_p$. As the course progresses, we will see examples of more sophisticated machine learning problems, often with nonlinear objective function and other **loss functions** instead of the least square error. "}
{"cell_type":"markdown","metadata":{},"source":"### <font color=\"grey\">A second example: linear programming</font>\n---\nSuppose a plane has two cargo compartments with weight capacities $C_1=35$ and $C_2=40$ tonnes, and volumes (space capacities) $V_1=250$ and $V_2=400$ cubic metres. Assume we have three types of cargo to transport, specified as follows.\n \n |          | Volume (m$^3$ per tonne) | Weight (tonnes) | Profit (£ / tonne)|\n |---------:|--------------------------|-----------------|-------------------------| \n | Cargo 1  |   8                      |  25             | £ 300           |\n | Cargo 2  |  10                      |  32             | £ 350           |\n | Cargo 3  |   7                      |  28             | £ 270           |\n  \n The problem is now to decide how much of each cargo to take on board, and how to distribute it in an optimal way among the two compartments.\n 1. The **decision variables** $x_{ij}$ specify the amount, in tonnes, of cargo $i$ to go into compartment $j$. We collect them in a vector $\\vct{x}$.\n 2. The **objective function** is the total profit, \n  \\begin{equation*}\n   f(\\vct{x}) = 300\\cdot (x_{11}+x_{12})+ 350\\cdot (x_{21}+x_{22})+270\\cdot (x_{31}+x_{32}).\n  \\end{equation*}\n 3. The **constraints** are given by the space and weight limitations of the compartments, and the amount of cargo available.\n\\begin{align*}\n x_{11}+x_{12} & \\leq 25 \\quad \\text{ (total amount of cargo 1)}\\\\ \n x_{21}+x_{22} & \\leq 32 \\quad \\text{ (total amount of cargo 2)}\\\\ \n x_{31}+x_{32} & \\leq 28 \\quad \\text{ (total amount of cargo 3)}\\\\\n x_{11}+x_{21}+x_{31} & \\leq 35 \\quad \\text{ (weight constraint on compartment 1)}\\\\\n x_{12}+x_{22}+x_{32} & \\leq 40 \\quad \\text{ (weight constraint on compartment 2)}\\\\\n 8x_{11}+10x_{21}+7x_{31} & \\leq 250 \\quad \\text{ (volume constraint on compartment 1)}\\\\\n 8x_{12}+10x_{22}+7x_{32} & \\leq 400 \\quad \\text{ (volume constraint on compartment 2)}\\\\\n (x_{11}+x_{21}+x_{31})/35 - (x_{12}+x_{22}+x_{32})/40 &= 0 \\quad \\text{ (maintain balance of weight ratio)}\\\\\n x_{ij} &\\geq 0 \\quad \\text{ (cargo can't have negative weight)}\n\\end{align*}\n\n It is customary to write the objective function as a scalar product, $f(\\vct{x}) = \\ip{\\vct{c}}{\\vct{x}} := \\vct{c}^{\\trans}\\vct{x}$, and to express the constraints as systems of linear equations and inequalities using matrix-vector products,\n \\begin{align*}\n  \\maximize &\\ip{\\vct{c}}{\\vct{x}} \\\\\n  \\subjto &A\\vct{x}\\leq \\vct{b}\\\\\n  & B\\vct{x} = \\vct{d}\\\\\n          & \\vct{x}\\geq 0        \n \\end{align*}\nwhere the inequalities $\\geq$ and $\\leq$ are to be understood componentwise.\n This problem has a unique solution that can be found using CVXPY in Python,\n "}
{"cell_type":"code","execution_count":87,"metadata":{"collapsed":false,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":"Solution found: \n [[  6.75   7.71   0.    32.    28.     0.  ]]\n"}],"source":"# Define all the matrices and vectors involved\nc = np.array([300,300,350,350,270,270])\nA = np.array([[1, 1, 0, 0, 0, 0],\n              [0, 0, 1, 1, 0, 0],\n              [0, 0, 0, 0, 1, 1],\n              [1, 0, 1, 0, 1, 0],\n              [0, 1, 0, 1, 0, 1],\n              [8, 0, 10, 0, 7, 0],\n              [0, 8, 0, 10, 0, 7]])\nb = np.array([25,32,28,35,40,250,400]);\nB = np.array([1/35, -1/40, 1/35, -1/40, 1/35, -1/40]);\nd = np.zeros(1)\n\n# Create variables, objective and constraints\nx = cvx.Variable(6)\nconstraints = [A*x <= b, B*x == d, x >= 0]\nobjective = cvx.Maximize(c*x)\nprob = cvx.Problem(objective, constraints)\n\n# Solve the optimization problem and output the result\nprob.solve()\nprint(\"Solution found: \\n\", np.round(np.abs(x.value), decimals=2).transpose())"}
{"cell_type":"markdown","metadata":{},"source":"In summary, the solution found is\n \\begin{equation*}\n x_{11} = 6.75 , x_{12} =  7.71, x_{21} = 0, x_{22} = 32, x_{31} = 28, x_{32} = 0.\n \\end{equation*}\n\n We made some simplifying assumptions, for example that the cargo can be split up into arbitrary fractions. Additional work is required to resolve these issues.\n Problems of this kind are known as **linear programming**, because the objective function and the constraints are given by linear functions. Such problems can be solved efficiently using the simplex algorithm or interior point methods. The highly developed theory of linear programming acts as a template for more general convex optimization that is developed in this course."}
{"cell_type":"markdown","metadata":{},"source":"Todo: visualise solution (or projection thereof)"}
{"cell_type":"markdown","metadata":{},"source":"### <font color=\"grey\">A third example: image inpainting</font>\n---\nOptimization methods play an increasingly important role in image and signal processing. "}
{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"trusted":true},"outputs":[],"source":""}