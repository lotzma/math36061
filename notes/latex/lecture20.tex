%-----------------------------------------------------------------------
% Beginning of chap2.tex
%-----------------------------------------------------------------------
%
%  AMS-LaTeX sample file for a chapter of a monograph, to be used with
%  an AMS monograph document class.  This is a data file input by
%  chapter.tex.
%
%  Use this file as a model for a chapter; DO NOT START BY removing its
%  contents and filling in your own text.
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter*{Lecture 20}
\addcontentsline{toc}{chapter}{Lecture 20}
\setcounter{chapter}{20}
\setcounter{section}{0}
\setcounter{equation}{0}
\setcounter{theorem}{0}
%\numberwithin{section}{chapter}
\numberwithin{equation}{chapter}
\numberwithin{theorem}{chapter}

%\epigraph{}{--- \textup{}}

The standard form of a semidefinite programming problem is

\begin{align}\label{eq:sdp-p}\tag{SDP-P}
\begin{split}
 \minimize & \mtx{C}\bullet \mtx{X}\\
 \subjto & \mtx{A}_i\bullet \mtx{X} = b_i, \quad 1\leq i\leq m,\\
 & \mtx{X} \succeq \zerovct,
 \end{split}
\end{align}
where $\mtx{C}$ and $\mtx{A}_i$, $1\leq i\leq m$, are symmetric $n\times n$ matrices (elements of the vector space $\mathrm{SYM}_n$). At first sight, it is not clear why solving such problem should be of interest. In this lecture we discuss a useful application to {\em discrete} optimization problems that are usually difficult in practice.

\section{Semidefinite relaxation}
A graph is a pair $G=(V,E)$, where $V=\{1,\dots,n\}$ consists of the {\em vertices}, and $E\subseteq V\times V$ consists of {\em edges} connecting some of the vertices. We will consider undirected edges, i.e., $(i,j)$ will be the same edge as $(j,i)$. 
\begin{figure}[h!]
\centering
\begin{tikzpicture}[scale=0.5]
 
  \useasboundingbox (-6.5,-3) rectangle (5.5,3);
 
  \begin{scope}[xshift=-9cm,yshift=-1cm]
    \foreach \x in {4,2,0} {
      \foreach \y in {0,2,4} {
        \drawLinewithBG{\x,0}{\y,2};
      }
    }
 
    \foreach \x in {0,2,4} {
      \foreach \y in {0,2} {
        \node at (\x,\y) [circle,fill=black] {};
      }
    }
 
  \end{scope}
 
  \begin{scope}[xshift=0cm]
    \foreach \a in { 18, 90, 162, 234, 306 } {
      \foreach \b in { 18, 90, 162, 234, 306 } {
        \drawPolarLinewithBG{\a:2}{\b:2};
      }
    }
 
    \foreach \a in {18,90, 162, 234, 306 } {
      \node at (\a:2cm) [circle,fill=black] {};
    }
  \end{scope}
 
 \begin{scope}[xshift=6cm,yshift=-1.5cm]
  % Arrow placement
  \drawBG{-1,0}{-1,2};
  \drawBG{-1,2}{0,3};
  \drawBG{0,3}{1,2};
  \drawBG{1,2}{1,0};
  \drawBG{1,0}{-1,2};
  
  % Node placement
  \node at (-1,2)   [circle, fill=black] {};
  \node (2) at (-1,0)  [mynode] {};
  \node (3) at (1,2)   [mynode] {};
  \node (4) at (1,0)   [mynode] {};
  \node (5) at (0,3)   [mynode] {};
 \end{scope}

\end{tikzpicture}
\caption{Some graphs}
\end{figure}

Note that for graphs, we only care about the incidences (which node is connected to which other node), the images are just visualizations and the distances and placements of nodes are pure convenience.
Graphs are an important tool in areas such as network analysis, and many discrete computational problems can be formulated as graph problems. One such problem is \textsc{MaxCut}. Given a graph $G=(V,E)$, a {\em cut} is a pair $(S,V\backslash S)$, where $S\subseteq V$ is a subset of vertices. The {\em edge set} of the cut is the set
\begin{equation*}
 E(S,V\backslash S) = \{e\in E\mid e \text{ connects a vertex in } S \text{ with a vertex in } V\backslash S\}.
\end{equation*}
The size of a cut is the number of edges in it, and the \textsc{MaxCut} problem is the problem of finding a cut of maximal size. 

\begin{example}
 Consider the graph with $V=\{1,2,3,4\}$ and $E=\{(1,2),(3,4),(1,4)\}$.
 \begin{figure}[h!]
  \centering
  \begin{tikzpicture}[scale=0.5]
    % Arrow placement
  \drawBG{-1,0}{-1,2};
  \drawBG{-1,2}{1,0};
  \drawBG{1,2}{1,0};
  
  % Node placement
  \node at (-1,2)   [circle, fill=black,label=135:$1$] {};
  \node (2) at (-1,0)  [mynode,label=225:$2$] {};
  \node (3) at (1,2)   [mynode,label=45:$3$] {};
  \node (4) at (1,0)   [mynode,label=-45:$4$] {};
  \end{tikzpicture}
 \end{figure}
 There are $7$ nontrivial cuts, namely
 \begin{equation*}
  \{1\}, \{2\}, \{3\}, \{4\}, \{1,2\}, \{1,3\}, \{1,4\}
 \end{equation*}
Note that we don't list all the non-empty subsets of $\{1,2,3,4\}$, as that would amount to counting every cut twice (every set would feature as $S$ and as $V\backslash S$).
The sizes of these cuts (number of edges joining $S$ with $V\backslash S$) is
\begin{equation*}
 2, 1, 1, 2, 1, 3, 2.
\end{equation*}
The maximum cut size is therefore $3$ (it can't be any bigger, since we only have three edges), and one such cut can be realized by taking the set $S=\{1,3\}$ and $V\backslash S=\{2,4\}$. 
\end{example}

The problem \textsc{MaxCut} (like many other graph problems) has the property that it is NP-hard. This means that the decision version of this problem (given a cut, does it have size at least $k$?) is NP-complete, which in turns means that checking the statement is easy (just count the number of edges in the cut), but {\em finding} a cut of size at least $k$ appears to be difficult: there does not seem to be a way of doing this that is fundamentally faster than to test all the possible cuts. A famous conjecture, P$\neq$NP, implies that there does not exist an efficient (polynomial time) algorithm that would, in general, be able to find a cut of a given size, or even a maximal cut. 

Two common approaches to deal with difficult problems are {\em approximation} and {\em randomization}. In approximation, one gives up on finding the best solution and concentrates on finding one that is good enough. In randomization, one takes into account that an algorithm may fail with small probability. Both approximation and randomization may lead to efficient algorithms that can solve a problem well enough for ``all practical purposes''. 

In the case of \textsc{MaxCut}, an approximation algorithm $\mathcal{A}$ takes as input a graph $G$ and outputs a set $S\subseteq V$, written as $\mathcal{A}(G)=S$. If by $\omega(S)$ we denote the size of the cut induced by $S$, and by $\mathrm{Opt}(G)$ the largest size of a cut, then the algorithm $\mathcal{A}$ is said to provide an $\delta$-approximation (for some $\delta>0$), if
\begin{equation*}
 \omega(\mathcal{A}(G))\geq \delta \cdot \mathrm{Opt}(G).
\end{equation*}
In words, the algorithms provides a cut that is optimal up to a factor $\delta$. A randomized algorithm is one that is allowed to use internal coin flips. Such an algorithm is a $\delta$-approximation algorithm, if the expected value
\begin{equation*}
 \mathrm{E}[\omega(\mathcal{A}(G))] \geq \delta \cdot \mathrm{Opt}(G).
\end{equation*}
We are interested in approximation algorithms that run in {\em polynomial time}. This means that the number of computational steps is a polynomial in the input size. Interior point methods for convex optimization and semidefinite programming are examples of polynomial time algorithms, and it is a remarkable fact that \textit{MaxCut} can be solved approximatively using semidefinite programming.

\section{The Goemans-Williamson Algorithm}
The \textsc{MaxCut} problem can be formulated as an integer programming problem as follows. Introduce variables $x_1,\dots,x_n$, where $n$ is the number of vertices. A cut is then an assignment $x_i=1$ (meaning that $i\in S$) or $x_i=-1$ (meaning that $i\in V\backslash S$). If $e=(i,j)$ is an edge in the cut, then $x_ix_j=-1$, and $x_ix_j=1$ otherwise. We therefore have
\begin{equation*}
 \frac{1}{2}(1-x_ix_j) = \begin{cases}
                          1 & \text{ if } (i,j) \in E(S,V\backslash S)\\
                          0 & \text{ if } (i,j) \not\in E(S,V\backslash S).
                         \end{cases}
\end{equation*}
The size of a cut is then the sum $\sum_{(i,j)\in E} \frac{1}{2}(1-x_ix_j)$, and the problem of finding the maximum cut can be written as
\begin{align}\label{eq:maxcut-orig}
\begin{split}
 \maximize & \sum_{(i,j)\in E} \frac{1}{2}(1-x_ix_j)\\
 \subjto & x_i^2=1, \ 1\leq i\leq n.
\end{split}
 \end{align}
We now look for a {\em relaxation} of the problem: a new problem with a bigger constraint set that also contains the solutions of~\eqref{eq:maxcut-orig}, but possibly more. As a first step, replace $x_i\in \{-1,1\}$ with vectors $\vct{u}_i\in \{\pm \vct{e}_1\}$, where $\vct{e}_1=(1,0,\dots,0)^{\trans}$ is a unit vector, and $x_ix_j$ with $\vct{u}_i^{\trans}\vct{u}_j$. The optimization is then over sets of vectors $(\vct{u}_1,\dots,\vct{u}_n)$, and can be written as
\begin{align*}
\maximize & \sum_{(i,j)\in E} \frac{1}{2}(1-\vct{u}_i^{\trans}\vct{u}_j)\\
\subjto & \vct{u}_i\in \{\pm \vct{e}_1\}.
\end{align*}
The relaxation now consists in enlarging the constraint set to allow for {\em any} unit vectors $\vct{u}_i\in S^{n-1}$, where $S^{n-1}=\{\vct{x}\mid \norm{\vct{x}}=1\}$ is the unit sphere.
\begin{align}\label{eq:relax1}
\begin{split}
\maximize & \sum_{(i,j)\in E} \frac{1}{2}(1-\vct{u}_i^{\trans}\vct{u}_j)\\
\subjto & \norm{\vct{u}_i}=1.
\end{split}
\end{align}
As the constraint set of the relaxation is bigger, the solution will be at least as large as the solution of~\eqref{eq:maxcut-orig} (ideally, the same, but we can't guarantee this).

For any set of unit vectors $\vct{u}_i$, let $\mtx{U}=(\vct{u}_1,\dots,\vct{u}_n)$ be the matrix with unit vectors $\vct{u}_i$ as columns, and consider
\begin{equation*}
 \mtx{X}=\mtx{U}^{\trans}\mtx{U}.
\end{equation*}
Then $x_{ii}=\vct{u}_i^{\trans}\vct{u}_i=1$ and the matrix $\mtx{X}$ is symmetric and positive definite.
Conversely, any symmetric positive definite matrix $\mtx{X}$ can be written as $\mtx{X}=\mtx{U}^{\trans}\mtx{U}$ (Cholesky factorization), and if in addition $x_{ii}=1$, then the columns of $\mtx{U}$ necessarily have unit length. We conclude that Problem~\eqref{eq:relax1} is equivalent to the following SDP:
\begin{align}\label{eq:relax-sdp}
\begin{split}
\maximize & \sum_{(i,j)\in E} \frac{1}{2}(1-x_{ij})\\
\subjto & x_{ii}=1\\
& \mtx{X}\succeq \zerovct.
\end{split}
\end{align}
Write $\mathrm{SDP}(G)$ for the optimal value of~\eqref{eq:relax-sdp}. From the above discussion we have
\begin{equation*}
 \mathrm{SDP}(G)\geq \mathrm{Opt}(G).
\end{equation*}
This is the case, because the solution of~\eqref{eq:maxcut-orig} is contained in the feasible set of~\eqref{eq:relax-sdp}, but the latter has more options.

To recover the cut, we proceed as follows.
\begin{enumerate}
 \item Solve~\eqref{eq:relax-sdp} to accuracy $\varepsilon$ to obtain a matrix $\mtx{X}^*$ such that
 \begin{equation*}
  \sum_{(i,j)\in E} \frac{1}{2}(1-x_{ij}^*) \geq \mathrm{SDP}(G)-\varepsilon.
 \end{equation*}
 This can be done in polynomial time using, for example, interior point methods.
\item Perform a Cholesky factorization $\mtx{X}^*=(\mtx{U}^*)^{\trans}\mtx{U}^*$ and extract the columns
$\vct{u}_1^*,\dots,\vct{u}_n^*$, which are unit vectors and satisfy
\begin{equation*}
 \sum_{(i,j)\in E} \frac{1}{2}(1-(\vct{u}_i^*)^{\trans}\vct{u}_j^*)\geq \mathrm{Opt}(G)-\varepsilon.
\end{equation*}
\item Finally, we want a way to recover a partition of the graph without loosing too much. We thus need to assign to each $\vct{u}_i$ a value $x_i\in \{-1,1\}$ according so some rule, and declare the result to be our partition. To do so, we {\em randomly} choose a vector $\vct{p}\in S^{n-1}$ and then set
\begin{equation*}
 x_i = \begin{cases}
        1 & \text{ if } \vct{u}_i^{\trans}\vct{p} \geq 0,\\
        -1 & \text{ else.}
       \end{cases}
\end{equation*}
\end{enumerate}
There we have our algorithm: it generates a cut. Let's call the above algorithm, which takes a graph $G$ and produces a set $S$ by the above steps, $\mtx{A}$.

\begin{theorem}
 The Goemans-Williamson algorithms generates a cut $\mathcal{A}(G)=S$ and satisfies
 \begin{equation*}
  \mathrm{E}[\omega(\mathcal{A}(G))] \geq 0.8785672 \cdot \mathrm{Opt}(G).
 \end{equation*}
\end{theorem}

That is, the algorithm is expected to generate a cut that is at least as $0.8785672$ times as good as the optimal cut (that is, could be about $10$ percent smaller but not more). By the law of large numbers, repeating the algorithm will likely give a result close to the expectation.




% %-----------------------------------------------------------------------
% % End of chap1.tex
% %-----------------------------------------------------------------------
