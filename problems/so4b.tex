\documentclass{article}
% Include macros here
\input{macros}
\usepackage{fancyhdr}
%\include{macros}
\usepackage{pifont}

% Number of problem sheet
\newcounter{problemSheetNumber}
\setcounter{problemSheetNumber}{4}
\newcommand{\matlabprob}{\ding{100} \ }
\newcommand{\examprob}{\ding{80} \ }
%\setcounter{section}{\theproblemSheetNumber}  
%\renewcommand{\theparagraph}{(\thesection.\arabic{paragraph})}
\newcounter{problems}
\setcounter{problems}{3}
%\setlength{\parindent}{0cm}
\renewcommand{\problem}[1]{\paragraph{(\theproblemSheetNumber.\theproblems)}\addtocounter{problems}{1}\label{#1}}
\renewcommand{\solution}[1]{\paragraph{Solution (\theproblemSheetNumber.\theproblems)}\addtocounter{problems}{1}\label{#1}}

\pagestyle{fancy}
\lhead{MATH36061}
\chead{Convex Optimization}
\rhead{October 24, 2016}

\begin{document} 
\begin{center}
{\Large {\bf Solutions to Part B of Problem Sheet \theproblemSheetNumber}}
\end{center}

\solution{pr:1} 
\begin{itemize}
 \item[(a)] Given complex numbers $z_1=a+ib$ and $z_2=c+id$, we can express the real and imaginary parts of the product $z_3=z_1z_2$ as
 \begin{equation*}
  \begin{pmatrix}
   \mathrm{re}(z_3)\\
   \mathrm{im}(z_3)
  \end{pmatrix}=
  \begin{pmatrix}
   a & -b\\
   b & a
  \end{pmatrix}
\begin{pmatrix}
 c \\ d
\end{pmatrix}.
 \end{equation*}
In the same fashion, a system of equations $\mtx{A}\vct{x}=\vct{b}$, with $\mtx{A}$ and $\vct{x}$ complex, we can be written as
\begin{equation*}
 \begin{pmatrix}
  \mathrm{re}(\vct{b})\\
  \mathrm{im}(\vct{b})
 \end{pmatrix}=
 \begin{pmatrix}
  \mathrm{re}(\mtx{A}) & -\mathrm{im}(\mtx{A})\\
  \mathrm{im}(\mtx{A}) & \mathrm{re}(\mtx{A})
 \end{pmatrix}
 \begin{pmatrix}
  \mathrm{re}(\vct{c})\\
  \mathrm{im}(\vct{c})
 \end{pmatrix}.
\end{equation*}
Since we know that the target vector $\vct{b}$ is real, we only need the upper half of this system. Once this is solved, we can assemble the complex $\vct{c}$ from it.

\item[(b)+(c)] The code could look something like this:

\begin{ipythonnb}
import numpy as np
import numpy.linalg as la
import numpy.random as rnd
import numpy.fft as fft
import matplotlib.pyplot as plt
import cvxpy as cvx
\end{ipythonnb}

\begin{ipythonnb}
def f(x):
    return 1.7*np.sin(30.*x)+0.5*np.cos(9.*x)+0.5*np.sin(6.*x)
    -np.cos(11.*x)+0.2*np.sin(13.*x)
\end{ipythonnb}

\begin{ipythonnb}
n = 512
T = 2*np.pi/n
xx = np.linspace(0,2*np.pi-T,n)
yy = f(xx)
% matplotlib inline
plt.plot(xx,yy,linewidth=3)
plt.show()
\end{ipythonnb}

\includegraphics[width=0.5\textwidth]{images/41.png}

\begin{ipythonnb}
m = 30
p = rnd.permutation(n)
points = xx[p[:m]]
samples = f(points)
plt.plot(xx,yy,linewidth=2)
plt.plot(points,samples,'o', color='red')
plt.show()
\end{ipythonnb}

The red dots indicate the points that we see. We know nothing else about the signal!

\includegraphics[width=0.5\textwidth]{images/42.png}

We now show how to reconstruct the whole blue curve from the knowledge of the red dots alone. We do this by setting up an optimization problem of the form

\begin{equation*}
  \minimize \|\vct{x}\|_1 \quad \subjto \mtx{A}\vct{x}=\vct{b}
\end{equation*}

for suitable matrix $\mtx{A}$ and vector $\vct{b}$. How $\mtx{A}$ and $\vct{b}$ are constructed is described in the problem description. Below is the implementation.

\begin{ipythonnb}
D = fft.ifft(np.eye(n))
rD = np.concatenate((D.real, D.imag), axis=1)
A = rD[p[:m],:]
fy = fft.fft(yy)
b = np.dot(A,np.concatenate((fy.real, fy.imag), axis=0))
\end{ipythonnb}

\begin{ipythonnb}
x = cvx.Variable(2*n)
constraints = [A*x == b]
obj = cvx.Minimize(cvx.norm(x,1))
prob = cvx.Problem(obj, constraints)
prob.solve()

x = np.array(x.value).transpose()[0]
\end{ipythonnb}

\begin{ipythonnb}
newy_im = fft.ifft(x[:n]+1j*x[n:])
newy = newy_im.real
print la.norm(newy-yy,1)

plt.subplot(2,1,1)
plt.plot(xx,yy,linewidth=3)
plt.subplot(2,1,2)
plt.plot(xx,newy,linewidth=3)
plt.show()
\end{ipythonnb}

\includegraphics[width=0.8\textwidth]{images/43.png}

The error obtained is of order $10^{-7}$. Now the interesting question is: \textbf{how much undersampling can we get away with?} To find out, we can repeat the previous experiment with values of m between $1$ and $512$ and find out where the method starts working. Obviously sampling only one point will not work (not enough information), and sampling all 512 points will work (we have all the information). As we say, $30$ points is already sufficient, but can we do with less?


\end{itemize}
\end{document}
