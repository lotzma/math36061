\documentclass{article}
% Include macros here
\input{macros}
\usepackage{fancyhdr}
%\include{macros}
\usepackage{pifont}

% Number of problem sheet
\newcounter{problemSheetNumber}
\setcounter{problemSheetNumber}{8}
\newcommand{\matlabprob}{\ding{100} \ }
\newcommand{\examprob}{\ding{80} \ }
%\setcounter{section}{\theproblemSheetNumber}  
%\renewcommand{\theparagraph}{(\thesection.\arabic{paragraph})}
\newcounter{problems}
\setcounter{problems}{0}
%\setlength{\parindent}{0cm}
\renewcommand{\problem}{\paragraph{(\theproblemSheetNumber.\theproblems)}\addtocounter{problems}{1}}

%\theoremstyle{remark}
%\newtheorem{problem}[problemSheetNumber]{}

\pagestyle{fancy}
\lhead{MATH36061}
\chead{Convex Optimization}
\rhead{December 3, 2016}

\begin{document} 
\begin{center}
{\Large {\bf Problem Sheet \theproblemSheetNumber}}
\end{center}

Problems in Part A will be discussed in class. Problems in Part B come with solutions and should be tried at home. 


\section*{Part A}
\problem Given a matrix $\mtx{A}\in \R^{m\times n}$ and $\vct{b}\in \R^m$, formulate the first-order optimality conditions for the problem 
\begin{equation*}
 f(\vct{x}) = -\sum_{i=1}^m \log(b_i-\mtx{a}_i^{\trans}\vct{x}),
\end{equation*}
with the constraints $\mtx{A}\vct{x}+\vct{s}=\vct{b}$ and $\vct{s}> \zerovct$. Compute the Lagrange dual.

\problem Consider the optimization problem
 \begin{equation}\label{eq:ex1}\tag{1}
  \minimize x_1^2+x_2^2 \quad \subjto \frac{x_1}{1+x_2^2}\leq 0, \ (x_1+x_2)^2=0.
 \end{equation}
Show that this problem is not a convex optimization problem. Derive a convex optimization problem  that has the same solution as~\eqref{eq:ex1}



\problem A quadratically constraint quadratic problem (QCQP) has the form
\begin{align*}
 \minimize & \frac{1}{2} \mtx{x}^{\trans}\mtx{P}\mtx{x}+\vct{q}^{\trans}\vct{x}+r\\
 \subjto & \frac{1}{2}\vct{x}^{\trans}\mtx{P}_i\vct{x}+\vct{q}_i^{\trans}\vct{x}+r_i\leq 0, \ 1\leq i\leq m,
\end{align*}
with $\mtx{P}$ symmetric positive definite and $\mtx{P}_1,\dots,\mtx{P}_m$ symmetric positive semidefinite. Derive the Lagrange dual of this problem.

%\problem A matrix
%\begin{equation*}
% \mtx{A} = \begin{pmatrix}
%            \mtx{B} & \vct{v}\\
%            \vct{v}^{\trans} & b
%           \end{pmatrix},
%\end{equation*}
%is positive definite if and only if $b-\vct{v}^{\trans}\mtx{B}^{-1}\vct{v}\geq 0$.
%Use this, and the fact that a symmetric matrix factors as $\mtx{A}=\vct{M}^{\trans}\vct{M}$ for some $\mtx{M}$, to show that above QCQP can be formulated as a semidefinite programming problem. 

%\problem Consider the general convex optimization problem
%\begin{align*}
%\begin{split}
% \minimize & f(\vct{x})\\
% \subjto & \vct{f}(\vct{x})\leq \zerovct\\
%         & \mtx{A}\vct{x} = \vct{b}.
%\end{split}
%\end{align*}
%The central path consists of the set of solutions $\vct{x}(t)$, $t>0$, of the barrier problem
%\begin{align*}
%\begin{split}
%\minimize & tf(\vct{x})+\phi(\vct{x})\\
%\subjto & \mtx{A}\vct{x}=\vct{b},
%\end{split}
%\end{align*}
%with $\phi(\vct{x}) = -\sum_{i=1}^m \log(-f_i(\vct{x}))$ is the logarithmic barrier function.
%Show that a point $\vct{x}$ is equal to a point $\vct{x}^*(t)$ on the central path if and only if there exist dual multipliers $\lambda$ and $\mu$ such that the following conditions are satisfied.
% \begin{align*}
%\begin{split}
%  \vct{f}(\vct{x}^*) & \leq \zerovct\\
%  \mtx{A}\vct{x}^* & = \vct{b}\\
%  \vct{\lambda}^*&\geq \zerovct\\
%  -\lambda_i^*f_i(\vct{x}^*) & =\frac{1}{t}, \ 1\leq i\leq m\\
%  \nabla_{\vct{x}} f(\vct{x}^*)+\sum_{i=1}^m \lambda_i^* \nabla_{\vct{x}}f_i(\vct{x}^*)+\mtx{A}^{\trans}\vct{\mu}^* &= \zerovct,
% \end{split}
% \end{align*}
% 
\newpage
 \section*{Part B}

%\problem Given a symmetric matrix $\mtx{A}$, formulate the problem of computing the largest eigenvalue $\lambda_{\mathrm{max}}(\mtx{A})$ as a semidefinite programming problem.
%

\problem Consider the {\em Boolean} optimization problem
\begin{align*}
\minimize & \ip{\vct{c}}{\vct{x}} \\
\subjto & \mtx{A}\vct{x}\leq \vct{b}\\
& x_i \in \{0,1\}, \ 1\leq i\leq n.
\end{align*}
This problem requires the $x_i$ to have integer values, and falls outside the scope of continuous optimization. Show that the problem is equivalent to
\begin{align*}
\minimize & \ip{\vct{c}}{\vct{x}} \\
\subjto & \mtx{A}\vct{x}\leq \vct{b}\\
& x_i(1-x_i)=0, \ 1\leq i\leq n.
\end{align*}
While this problem is not convex (the equality constraints are quadratic), we can still formulate the Lagrange dual to this problem, whose optimal value gives a lower bound. Show that the Lagrange dual is a convex optimization problem, thus giving a way to {\em approximate} the solution of the discrete problem by solving a convex optimization problem.

\problem Consider the problem
\begin{equation}\label{eq:1}\tag{2}
 \minimize \vct{x}^{\trans}\mtx{W}\mtx{x} \quad \subjto x_i^2=1, \ 1\leq i\leq n
\end{equation}
for a symmetric matrix $\mtx{W}$.
The feasible points are the sets of vectors $\vct{x}\in \{-1,1\}^n$, with each coordinate either $-1$ or $1$. In principle, we can solve this problem by testing the objective function $\vct{x}^{\trans}\mtx{W}\vct{x}$ on all $2^n$ such problems, but it is computationally inefficient to do so. An interpretation of this problem is as follows: we want to group $n$ elements into two groups, one labeled with $-1$ and one with $1$. The entry $w_{ij}$ of the matrix can be seen as the cost of having $i$ and $j$ in the same partition.

Using Lagrangian duality, show that the optimal value $p^*$ of~\eqref{eq:1} satisfies
\begin{equation*}
 p^* \geq n \cdot \lambda_{\mathrm{min}}(\mtx{W}),
\end{equation*}
where $\lambda_{\mathrm{min}}(\mtx{W})$ is the smallest eigenvalue of $\mtx{W}$.

%\problem In many applications one is interested in finding a matrix of low rank that satisfies certain constraints. For example, one could have a covariance matrix, or a matrix containing user ratings of products, or a matrix whose entries are the squared distances between objects, but where only some entries are known. A common heuristic is to replace the rank of a symmetric matrix with the sum of the eiganvalues
%
%\begin{itemize}
% \item[(a)] Show that for a symmetric matrix $\mtx{A}$, the sum of the eigenvalues $\lambda_1+\cdots+\lambda_n$ equals the trace $\mathrm{tr}(\mtx{A})$. We can therefore write
% \begin{equation*}
%  \lambda_1+\cdots+\lambda_n = \mathrm{tr}(\mtx{A}) = \mtx{I}\bullet \mtx{A}.
% \end{equation*}
%\item[(b)] Formulate the problem of minimizing the trace of a symmetric positive semidefinite matrix $\mtx{X}$ subject to constraints of the form
%\begin{equation*}
% x_{ij} = a_{ij}
%\end{equation*}
%for some subset of indices $(i,j)\in \Omega \subseteq \{1,\dots,n\}^2$. The problem is that of finding the matrix of smallest trace with some predetermined entries. Determine the dual of this problem.
%\item[(c)] Using CVX, perform the following experiment:
%\begin{itemize}
%\item Generate a random matrix $\mtx{X}_0\in \mathrm{SYM}_{100}$ of rank $10$. 
%\item For an increasing subcollection of ``known'' entris from $\vct{X}_0$, solve the trace minimization problem and determine if the solution of this optimization problem coincides with the matrix $\mtx{X}_0$, thus effectively recovering it from only limited information.
%\end{itemize}
%\end{itemize}


\end{document}
